# CrewAI Multi-Agent SA Call Analyzer Configuration
# This app uses 4 specialized AI agents to analyze Solution Architect performance

# ===== OPTION 1: Use Anthropic API (Recommended) =====
# Sign up at https://console.anthropic.com/
ANTHROPIC_API_KEY=your_api_key_here

# Model Selection (affects cost and quality)
# - claude-3-5-haiku-20241022   (~$0.25-0.50 per call) - Fast, cost-effective, great quality
# - claude-3-5-sonnet-20241022  (~$1.50-2.50 per call) - Best quality, deeper insights
MODEL_NAME=claude-3-5-haiku-20241022

# ===== OPTION 2: Use LiteLLM Proxy (Local/Self-hosted) =====
# Set USE_LITELLM=true to use your local LiteLLM instance instead of Anthropic
USE_LITELLM=false

# LiteLLM configuration (only needed if USE_LITELLM=true)
LITELLM_BASE_URL=http://localhost:4000
LITELLM_API_KEY=dummy

# When using LiteLLM, set MODEL_NAME to whatever model you configured:
# Examples:
# - gpt-4o-mini               (via OpenAI, cheap and good)
# - gpt-3.5-turbo             (via OpenAI, very cheap)
# - ollama/llama3.1:70b       (free, runs locally via Ollama)
# - groq/llama-3.1-70b        (fast and free via Groq)
# - any model your LiteLLM proxy supports

# ===== Gong API Integration (Optional) =====
# Allows fetching transcripts directly from Gong by URL
# Get credentials from: Settings > API in Gong (requires Technical Admin)
GONG_ACCESS_KEY=your_gong_access_key_here
GONG_SECRET_KEY=your_gong_secret_key_here

# ===== Observability (Optional) =====
# OpenInference tracing with Arize AX
# Sign up at https://arize.com/ to get your credentials
ARIZE_API_KEY=your_arize_api_key_here
ARIZE_SPACE_ID=your_space_id_here

# Disable CrewAI's built-in tracing (we use Arize instead)
OTEL_SDK_DISABLED=false
CREWAI_TELEMETRY_OPT_OUT=true

# Additional CrewAI telemetry disablers (to prevent 401 errors)
CREWAI_DISABLE_TELEMETRY=true
CREWAI_STORAGE_DIR=~/.crewai
LITELLM_DROP_PARAMS=true

# Completely disable CrewAI's internal tracing to prevent conflicts with Arize
OTEL_TRACES_EXPORTER=none
OTEL_METRICS_EXPORTER=none
OTEL_LOGS_EXPORTER=none

# ===== Span Processor Configuration (Advanced) =====
# Control how telemetry data is processed before sending to Arize

# Output Cleaning: Extract JSON from verbose LLM responses and truncate large payloads
# Recommended: true (reduces costs and improves readability in Arize)
ARIZE_ENABLE_OUTPUT_CLEANING=true
ARIZE_MAX_OUTPUT_LENGTH=10000  # Max characters for LLM outputs
ARIZE_MAX_INPUT_LENGTH=5000     # Max characters for LLM inputs

# PII Redaction: Automatically redact sensitive data (SSN, credit cards, emails, etc.)
# Recommended: false for dev, true for production with sensitive data
ARIZE_ENABLE_PII_REDACTION=false

# Metadata Enrichment: Add deployment environment and version tags to all spans
# Recommended: true (helps with filtering and analysis in Arize)
ARIZE_ENABLE_METADATA_ENRICHMENT=true
ENVIRONMENT=development  # e.g., development, staging, production
APP_VERSION=1.0.0        # Your application version
